{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Time to model our data, we're going to be using a lot of different classifiers and trying to find which ones will be the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data with dummies and Lasso CV\n",
    "dummy_income = pd.read_csv('./data/dummy_income.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Features\n",
    "features = income._get_numeric_data().columns.drop(['wage', 'smpl_wgt', 'fnlwgt', 'log_age'])\n",
    "X = income[features]\n",
    "y = income['wage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score_classification(X, y, models: list):\n",
    "    # Split data into training and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "    \n",
    "    \n",
    "    # Creating empty df to add to later\n",
    "    models_df = pd.DataFrame(columns=['model', \n",
    "                                      'parameters', \n",
    "                                      'train_accuracy',\n",
    "                                      'train_f1',\n",
    "                                      'train_spec',\n",
    "                                      'train_sens',\n",
    "                                      'test_accuracy',\n",
    "                                      'test_f1',\n",
    "                                      'test_spec',\n",
    "                                      'test_sens'])\n",
    "    \n",
    "    for model in models:\n",
    "        # Create a pipeline to scale data and pass through model\n",
    "        pipe = Pipeline([\n",
    "            ('sc', StandardScaler()),\n",
    "            ('model', model) # Thanks Lisa Tagliaferri from Digitalocean.com https://www.digitalocean.com/community/tutorials/how-to-use-args-and-kwargs-in-python-3\n",
    "        ])\n",
    "\n",
    "        # Fitting the model\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_preds = pipe.predict(X_train)\n",
    "        y_test_preds = pipe.predict(X_test)\n",
    "        \n",
    "        # Scoring the models\n",
    "        train_score = pipe.score(X_train, y_train)\n",
    "        train_f1 = f1_score(y_train, y_train_preds)\n",
    "        test_score = pipe.score(X_test, y_test)\n",
    "        test_f1 = f1_score(y_test, y_test_preds)\n",
    "        \n",
    "        # Calculate train specificity and sensitivity\n",
    "        tn, fn, fp, tp = confusion_matrix(y_train, pipe.predict(X_train)).ravel()\n",
    "        train_spec = tn / (tn + fp)\n",
    "        train_sens = tp / (tp + fn)\n",
    "        \n",
    "        # Calculate test specificity and sensitivity\n",
    "        tn, fn, fp, tp = confusion_matrix(y_test, pipe.predict(X_test)).ravel()\n",
    "        test_spec = tn / (tn + fp)\n",
    "        test_sens = tp / (tp + fn)\n",
    "        \n",
    "        # Returning a dictionary of the information\n",
    "        model_row = {'model' : type(model).__name__, # Thanks Jonathan from Stack Overflow! https://stackoverflow.com/questions/52763325/how-to-obtain-only-the-name-of-a-models-object-in-scikitlearn\n",
    "                     'parameters' : model.get_params(),\n",
    "                     'train_accuracy' : train_score,\n",
    "                     'train_f1' : train_f1,\n",
    "                     'train_spec' : train_spec,\n",
    "                     'train_sens' : train_sens,\n",
    "                     'test_accuracy': test_score,\n",
    "                     'test_f1': test_f1,\n",
    "                     'test_spec' : test_spec,\n",
    "                     'test_sens' : test_sens}\n",
    "        \n",
    "        # Add new row to models_df\n",
    "        models_df = models_df.append(model_row, ignore_index=True)\n",
    "        \n",
    "        print(f'Done with {model}, moving on')\n",
    "        \n",
    "    return models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_models = [LogisticRegression(n_jobs=12),\n",
    "                        DecisionTreeClassifier(max_depth=6),\n",
    "                        BaggingClassifier(base_estimator = DecisionTreeClassifier(max_depth=6), \n",
    "                                          n_estimators=500, \n",
    "                                          n_jobs=12),\n",
    "                        RandomForestClassifier(max_depth=6, \n",
    "                                               n_estimators=1000, \n",
    "                                               n_jobs=12, \n",
    "                                               random_state=42),\n",
    "                        AdaBoostClassifier(n_estimators=350, \n",
    "                                           random_state=42),\n",
    "                        VotingClassifier([\n",
    "                                        ('logreg', LogisticRegression(n_jobs=12)),\n",
    "                                         ('dt', DecisionTreeClassifier(max_depth=6)),\n",
    "                                         ('bc', BaggingClassifier(base_estimator = DecisionTreeClassifier(max_depth=6), \n",
    "                                                           n_estimators=500, \n",
    "                                                           n_jobs=12)),\n",
    "                                        ('rfc', RandomForestClassifier(max_depth=6, \n",
    "                                                               n_estimators=1000, \n",
    "                                                               n_jobs=12, \n",
    "                                                               random_state=42)),\n",
    "                                        ('ab', AdaBoostClassifier(n_estimators=500, \n",
    "                                                           random_state=42))]),                                        \n",
    "                        SVC(C=0, random_state=42)\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of our baseline model is ~76%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_df = model_score_classification(X, y, classification_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all these models perform fairly well before GridSearching, so let's break these up and use `GridSearchCV` to find the best ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_df.to_csv('./data/general_models_dummy_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearching through `AdaBoostClassifier`, `SVC`, and `GradientNB`\n",
    "\n",
    "Thanks Eric Heidbreder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating AdaBoost Pipeline\n",
    "pipe_ab = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('ab', AdaBoostClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SVC Pipeline\n",
    "pipe_svc = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('svc', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "pipe_gb = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('gb', GaussianNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GaussianNB params\n",
    "params_gb = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating AdaBoost Params\n",
    "params_ab = {\n",
    "    'ab__n_estimators' : [2500, 3000],\n",
    "    'ab__random_state' : [42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SVC Params\n",
    "params_svc = {\n",
    "    'svc__C': [10],\n",
    "    'svc__degree': [2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating AdaBoost GridSearch\n",
    "grid_ab = GridSearchCV(pipe_ab, params_ab, cv=5, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SVC GridSearch\n",
    "grid_svc = GridSearchCV(pipe_svc, params_svc, cv=5, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GaussianNB GridSearch\n",
    "grid_gb = GridSearchCV(pipe_gb,\n",
    "                     param_grid = params_gb,\n",
    "                     cv = 5,\n",
    "                     verbose = 1,\n",
    "                     scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Commented out so the .csv doesn't get overwritten\n",
    "# model_params = {}\n",
    "# count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you really want to run this GridSearch again, it will take awhile\n",
    "grid_ab.fit(X_train, y_train)\n",
    "grid_svc.fit(X_train, y_train)\n",
    "grid_gb.fit(X_train, y_train)\n",
    "\n",
    "# Create a new dictionary entry with the best params used in the GridSearch Pipeline\n",
    "grid_ab.best_params_['best_params'] = grid_ab.best_params_\n",
    "grid_svc.best_params_['best_params'] = grid_svc.best_params_\n",
    "grid_gb.best_params_['best_params'] = grid_gb.best_params_\n",
    "\n",
    "# Create a new dictionary entry with the model used in the GridSearch Pipeline\n",
    "grid_ab.best_params_['model'] = grid_ab.estimator[1]\n",
    "grid_svc.best_params_['model'] = grid_svc.estimator[1]\n",
    "grid_gb.best_params_['model'] = grid_gb.estimator[1]\n",
    "\n",
    "# Create a new dictionary entry with the cv score from the GridSearch\n",
    "grid_ab.best_params_['cv_score'] = grid_ab.best_score_\n",
    "grid_svc.best_params_['cv_score'] = grid_svc.best_score_\n",
    "grid_gb.best_params_['cv_score'] = grid_gb.best_score_\n",
    "\n",
    "# Create a new dictionary entry with the train score from the GridSearch\n",
    "grid_ab.best_params_['train_score'] = grid_ab.score(X_train, y_train)\n",
    "grid_svc.best_params_['train_score'] = grid_svc.score(X_train, y_train)\n",
    "grid_gb.best_params_['train_score'] = grid_gb.score(X_train, y_train)\n",
    "\n",
    "# Create a new dictionary entry with the test score from the GridSearch\n",
    "grid_ab.best_params_['test_score'] = grid_ab.score(X_test, y_test)\n",
    "grid_svc.best_params_['test_score'] = grid_svc.score(X_test, y_test)\n",
    "grid_gb.best_params_['test_score'] = grid_gb.score(X_test, y_test)\n",
    "\n",
    "# Add each of these entries to the list\n",
    "count += 1\n",
    "model_params[f'model_{count}'] = grid_ab.best_params_\n",
    "count += 1\n",
    "model_params[f'model_{count}'] = grid_svc.best_params_\n",
    "count += 1\n",
    "model_params[f'model_{count}'] = grid_gb.best_params_\n",
    "\n",
    "# Create a DataFrame from the dictionary we created above\n",
    "model_df = pd.DataFrame.from_dict(model_params, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearching through `DecisionTreeClassifier` \n",
    "\n",
    "Thanks Irene Anibogwu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for decision trees to find best estimator and params\n",
    "gridcv = GridSearchCV(estimator = DecisionTreeClassifier(),\n",
    "                    param_grid = {'max_depth': [3, 5, 7, 10],\n",
    "                                  'min_samples_split': [5, 10, 15, 20],\n",
    "                                  'min_samples_leaf': [2, 3, 4, 5, 6, 7]},\n",
    "                    cv = 5,\n",
    "                    verbose = 1,\n",
    "                    n_jobs=-1)\n",
    "gridcv.fit(X_train, y_train)\n",
    "gridcv.best_estimator_\n",
    "gridcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model w/ best parameters.\n",
    "dt = DecisionTreeClassifier(max_depth = 7,\n",
    "                            min_samples_split = 20,\n",
    "                            min_samples_leaf = 4,\n",
    "                            random_state = 42)\n",
    "# Fit model.\n",
    "dt.fit(X_train, y_train)\n",
    "# Evaluate model.\n",
    "print(f'Score on training set: {dt.score(X_train, y_train)}')\n",
    "print(f'Score on testing set: {dt.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearching through `RandomForestClassifier`\n",
    "\n",
    "Thanks Josh Mizraji!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {}\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaffolding\n",
    "params_rf = {\n",
    "    'n_estimators' : [55,60,70], #number of trees\n",
    "    'max_features' : [None], \n",
    "    'max_depth' : [7,8,9]\n",
    "}\n",
    "#Instantiate Gridsearch\n",
    "gs_rf = GridSearchCV(RandomForestClassifier(), \n",
    "                 param_grid=params_rf,\n",
    "                 cv=5,\n",
    "                n_jobs=-1)\n",
    "#Fit\n",
    "gs_rf.fit(X_train, y_train)\n",
    "#this takes the best params dictionary and adds a column called score\n",
    "gs_rf.best_params_['score'] = gs_rf.best_score_\n",
    "#make a counter\n",
    "count +=1\n",
    "#create new column with best params\n",
    "model_params[f'model_{count}'] = gs_rf.best_params_\n",
    "#orient sideways\n",
    "model_df = pd.DataFrame.from_dict(model_params, orient='index')\n",
    "model_df\n",
    "#adapted from DSI lesson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier([\n",
    "    ('ab', AdaBoostClassifier(n_estimators=2500, random_state=42)),\n",
    "#     ('SVC', SVC(C=10, degree=2)),\n",
    "    ('bag', BaggingClassifier(n_estimators=2000,\n",
    "                             max_samples=300,\n",
    "                             max_features=len(features))),\n",
    "    ('rf', RandomForestClassifier(max_depth=9,\n",
    "                                 n_estimators=70)),\n",
    "    ('dt', DecisionTreeClassifier(max_depth = 7,\n",
    "                                  min_samples_split = 20,\n",
    "                                  min_samples_leaf = 4,\n",
    "                                  random_state = 42)),\n",
    "], n_jobs=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Voting Classifier on our best models as determined by GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in test data\n",
    "income_test = pd.read_csv('./data/test_dummied.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean test data\n",
    "test_data = pd.read_csv('./data/test_data.csv')\n",
    "\n",
    "# replace the ? with 'unknown'\n",
    "test_data.replace(' ?', \"unknown\", inplace = True )\n",
    "cat_columns_test = test_data.drop(columns = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week'], axis = 1).columns\n",
    "\n",
    "# Stripping whitespace from beginning of each value\n",
    "for column in cat_columns_test:\n",
    "    test_data[column] = test_data[column].apply(lambda x: x.strip())\n",
    "    \n",
    "#Save as a new csv\n",
    "test_data.to_csv('./data/test_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting predictions for Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Features\n",
    "features = [column for column in dummy_income.columns if column in income_test.columns] # Selects only the columns that are in income_test\n",
    "X = dummy_income[features]\n",
    "y = income['wage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = AdaBoostClassifier(n_estimators=2500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.fit(X, y) # Training the model on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ab.predict(income_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds, columns=['wage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.to_csv('./data/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
